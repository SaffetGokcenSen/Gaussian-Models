{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation For Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that there is a discrete generative model. A classification is to be made using this model. The probability density function of the output to be in the class $c$ is given as follows:\n",
    "$$\n",
    "p\\left (y=c|\\textbf{x}, \\boldsymbol{\\theta}\\right )=\\frac{p\\left (\\textbf{x}|y=c, \\boldsymbol{\\theta} \\right )p\\left (y=c|\\boldsymbol{\\theta} \\right )}{p\\left ( \\textbf{x}|\\boldsymbol{\\theta}\\right )}\n",
    "$$\n",
    "If $p\\left (\\textbf{x}|y=c, \\boldsymbol{\\theta} \\right )$ is given by a multivariate Gaussian distribution, then this classification is called the quadratic discriminant classification. The maximum likelihood estimator for the quadratic discriminant classifier is to be derived. Hence, the likelihood of the data should be written first. The likelihood of a single data sample $\\left (\\textbf{x}_{i}, y_{i} \\right )$ is as follows:\n",
    "$$\n",
    "p\\left (\\textbf{x}_{i}, y_{i}|\\boldsymbol{\\theta}\\right )=p\\left (\\textbf{x}_{i}|y_{i}, \\boldsymbol{\\theta} \\right ) p\\left (y_{i}|\\boldsymbol{\\theta} \\right )=\\prod_{c=1}^{C} p\\left (y_{i}|\\boldsymbol{\\theta}_{c} \\right )^{I\\left (y_{i}=c \\right )}\\prod_{c=1}^{C}p\\left (\\textbf{x}_{i}|y_{i}, \\boldsymbol{\\theta}_{c} \\right )^{I\\left (y_{i}=c \\right )}\n",
    "$$\n",
    "Assuming that the samples are independent and there are $N$ training samples, the likelihood of the data can now be written as follows:\n",
    "$$\n",
    "p\\left (D|\\boldsymbol{\\theta} \\right )=\\prod_{i=1}^{N} p\\left (\\textbf{x}_{i}, y_{i}|\\boldsymbol{\\theta} \\right )\n",
    "$$\n",
    "For the sake of avoiding any numerical underflow and computational simplicity, the logarithm of the above expression is taken to obtain the log-likelihood:\n",
    "$$\n",
    "\\log p\\left (D|\\boldsymbol{\\theta} \\right )=\\sum_{i=1}^{N} \\log p\\left (\\textbf{x}_{i}, y_{i}|\\boldsymbol{\\theta} \\right )=\\sum_{i=1}^{N} \\log \\left [\\prod_{c=1}^{C} p\\left (y_{i}|\\boldsymbol{\\theta}_{c} \\right )^{I\\left (y_{i}=c \\right )}\\prod_{c=1}^{C}p\\left (\\textbf{x}_{i}|y_{i}, \\boldsymbol{\\theta}_{c} \\right )^{I\\left (y_{i}=c \\right )}\\right ]=\n",
    "$$\n",
    "$$\n",
    "\\sum_{i=1}^{N} \\left [\\sum_{c=1}^{C} \\log \\left [ p\\left (y_{i}|\\boldsymbol{\\theta}_{c} \\right )^{I\\left (y_{i}=c \\right )}\\right ]+\\sum_{c=1}^{C}\\log \\left [p\\left (\\textbf{x}_{i}|y_{i}, \\boldsymbol{\\theta}_{c} \\right )^{I\\left (y_{i}=c \\right )}\\right ]\\right ]=\\sum_{i=1}^{N} \\left [\\sum_{c=1}^{C} I\\left (y_{i}=c \\right )\\log p\\left (y_{i}|\\boldsymbol{\\theta}_{c} \\right ) +\\sum_{c=1}^{C}I\\left (y_{i}=c \\right )\\log p\\left (\\textbf{x}_{i}|y_{i}, \\boldsymbol{\\theta}_{c} \\right )\\right ]\\Rightarrow\n",
    "$$\n",
    "$$\n",
    "\\log p\\left (D|\\boldsymbol{\\theta} \\right )=\\sum_{c=1}^{C} N_{c}\\log p\\left (y_{i}=c|\\boldsymbol{\\theta}_{c} \\right ) +\\sum_{c=1}^{C} \\sum_{i:y_{i}=c}\\log p\\left (\\textbf{x}_{i}|y_{i}, \\boldsymbol{\\theta}_{c} \\right )\n",
    "$$\n",
    "The first term is maximized if the maximum likelihood estimator (MLE) for the class occurrences which is\n",
    "$$\n",
    "\\pi_{c_{MLE}}=\\frac{N_{c}}{N}\n",
    "$$\n",
    "is plugged in place of $p\\left (y_{i}=c|\\boldsymbol{\\theta}_{c} \\right )$. $N_{c}$ is the number of occurrences of the class $c$. The second term is maximized if the maximum likelihood estimators for the means and covariances of the multivariate Gaussian distributions for the probability densities of the input features conditioned on the class label are used. The MLE's for the mean and covariances are as follows:\n",
    "$$\n",
    "\\boldsymbol{\\mu}_{c_{MLE}}=\\frac{1}{N_{c}}\\sum_{i:y_{i}=c} x_{i}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_{c_{MLE}} = \\frac{1}{N_{c}} \\sum_{i:y_{i}=c} \\left (\\textbf{x}_{i}-\\boldsymbol{\\mu}_{c_{MLE}} \\right )\\left (\\textbf{x}_{i}-\\boldsymbol{\\mu}_{c_{MLE}} \\right )^{T}\n",
    "$$\n",
    "The maximum likelihood estimation measure for $p\\left (y=c|\\textbf{x}, \\boldsymbol{\\theta}\\right )$ is calculated without computing $p\\left ( \\textbf{x}|\\boldsymbol{\\theta}\\right )$ since it is the same for all of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryGaussianData:\n",
    "    def __init__(self, dist_mean, nonsingular_matrix, dist_cov_eigvalues, num_of_samples):\n",
    "        # 1-d numpy array for the mean of the distribution\n",
    "        self.dist_mean = dist_mean\n",
    "        # a nonsingular matrix to create the eigenvector matrix of the covariance matrix for the distribution\n",
    "        self.nonsingular_matrix = nonsingular_matrix\n",
    "        # 1-d array with eigenvalues to be used for the generation of the covariance matrix for the distribution\n",
    "        self.dist_cov_eigvalues = dist_cov_eigvalues\n",
    "        # number of samples from the distribution\n",
    "        self.num_of_samples = num_of_samples\n",
    "        \n",
    "    def compute_cov_matrix(self):\n",
    "        nonsingular_matrix = self.nonsingular_matrix\n",
    "        q, r = np.linalg.qr(nonsingular_matrix)\n",
    "        # normalize the eigenvector matrix of the covariance matrix for the distribution\n",
    "        for i in range(0, q.ndim):\n",
    "            q[i, :] = q[i, :] / np.linalg.norm(q[i, :])\n",
    "        eigenvalue_matrix = np.diag(self.dist_cov_eigvalues)\n",
    "        dist_cov_matrix = np.matmul(np.matmul(q, eigenvalue_matrix), q.T)\n",
    "        \n",
    "        return dist_cov_matrix\n",
    "    \n",
    "    \n",
    "    def sample_data(self):\n",
    "        dist_cov_matrix = self.compute_cov_matrix() \n",
    "        \n",
    "        return np.random.default_rng().multivariate_normal(mean=self.dist_mean, cov=dist_cov_matrix, size=self.num_of_samples)\n",
    "\n",
    "\n",
    "class MLEComputation:\n",
    "    def __init__(self, train_input, pi_class_mle):\n",
    "        # input data for training\n",
    "        self.train_input = train_input\n",
    "        # mle for class density\n",
    "        self.pi_class_mle = pi_class_mle\n",
    "    \n",
    "    def gaussian_mle_estimates(self):\n",
    "        mean_mle = np.mean(self.train_input, axis=0)\n",
    "        num_of_samples = self.train_input.shape[0]\n",
    "        num_of_features = self.train_input.shape[1]\n",
    "        cov_mle = np.zeros((num_of_features, num_of_features))\n",
    "        for i in range(0, num_of_samples):\n",
    "            centered_sample = self.train_input[i, :]-mean_mle\n",
    "            cov_mle = cov_mle + np.outer(centered_sample, centered_sample)\n",
    "        cov_mle = cov_mle/num_of_samples\n",
    "        \n",
    "        return mean_mle, cov_mle\n",
    "    \n",
    "    def compute_class_density_measure(self, x):\n",
    "        mean_mle, cov_mle = self.gaussian_mle_estimates()\n",
    "        return (multivariate_normal.pdf(x, mean=mean_mle, cov=cov_mle))*self.pi_class_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean of the category 0 is:\n",
      "[1 2 3 4]\n",
      "nonsingular matrix for generating the eigenvector matrix of the covariance of the category 0 is:\n",
      "[[ 1.   2.   0.   0. ]\n",
      " [ 2.  -1.   0.   0. ]\n",
      " [ 0.   4.   3.   0. ]\n",
      " [ 0.   0.   2.5  5. ]]\n",
      "eigenvalues of the covariance matrix of the distribution for the category 0:\n",
      "[1.  2.4 3.  3.8]\n",
      "covariance matrix of the category 0\n",
      "[[ 2.84883485 -0.92441743 -0.45552178  0.27234043]\n",
      " [-0.92441743  1.46220871  0.22776089 -0.13617021]\n",
      " [-0.45552178  0.22776089  2.68470111 -0.17021277]\n",
      " [ 0.27234043 -0.13617021 -0.17021277  3.20425532]]\n",
      "verification for the eigenvalues of the covariance matrix of the category 0\n",
      "[1.  2.4 3.  3.8]\n"
     ]
    }
   ],
   "source": [
    "mean_0 = np.array([1, 2, 3, 4])\n",
    "print('the mean of the category 0 is:')\n",
    "print(mean_0)\n",
    "non_singular_0 = np.array([[1, 2, 0, 0], [2, -1, 0, 0], [0, 4, 3, 0], [0, 0, 2.5, 5]])\n",
    "print('nonsingular matrix for generating the eigenvector matrix of the covariance of the category 0 is:')\n",
    "print(non_singular_0)\n",
    "Lambda_0 = np.array([1, 2.4, 3, 3.8])\n",
    "print('eigenvalues of the covariance matrix of the distribution for the category 0:')\n",
    "print(Lambda_0)\n",
    "num_of_samples_0 = 1000\n",
    "category_0 = CategoryGaussianData(mean_0, non_singular_0, Lambda_0, num_of_samples_0)\n",
    "cov_0 = category_0.compute_cov_matrix() \n",
    "print('covariance matrix of the category 0')\n",
    "print(cov_0)\n",
    "print('verification for the eigenvalues of the covariance matrix of the category 0')\n",
    "print(np.linalg.eigvalsh(cov_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean of the category 1 is:\n",
      "[5 6 7 8]\n",
      "nonsingular matrix for generating the eigenvector matrix of the covariance of the category 1 is:\n",
      "[[ 1.   2.   0.   0. ]\n",
      " [ 2.  -1.   0.   0. ]\n",
      " [ 0.   4.   3.   0. ]\n",
      " [ 0.   0.   2.5  5. ]]\n",
      "eigenvalues of the covariance matrix of the distribution for the category 1:\n",
      "[1.5 2.8 3.3 4.6]\n",
      "covariance matrix of the category 1:\n",
      "[[ 3.43483283 -0.96741641 -0.55927052  0.44255319]\n",
      " [-0.96741641  1.98370821  0.27963526 -0.2212766 ]\n",
      " [-0.55927052  0.27963526  3.14954407 -0.27659574]\n",
      " [ 0.44255319 -0.2212766  -0.27659574  3.63191489]]\n",
      "verification for the eigenvalues of the covariance matrix of the category 1:\n",
      "[1.5 2.8 3.3 4.6]\n"
     ]
    }
   ],
   "source": [
    "mean_1 = np.array([5, 6, 7, 8])\n",
    "print('the mean of the category 1 is:')\n",
    "print(mean_1)\n",
    "non_singular_1 = np.array([[1, 2, 0, 0], [2, -1, 0, 0], [0, 4, 3, 0], [0, 0, 2.5, 5]])\n",
    "print('nonsingular matrix for generating the eigenvector matrix of the covariance of the category 1 is:')\n",
    "print(non_singular_1)\n",
    "Lambda_1 = np.array([1.5, 2.8, 3.3, 4.6])\n",
    "print('eigenvalues of the covariance matrix of the distribution for the category 1:')\n",
    "print(Lambda_1)\n",
    "num_of_samples_1 = 1000\n",
    "category_1 = CategoryGaussianData(mean_1, non_singular_1, Lambda_1, num_of_samples_1)\n",
    "cov_1 = category_1.compute_cov_matrix() \n",
    "print('covariance matrix of the category 1:')\n",
    "print(cov_1)\n",
    "print('verification for the eigenvalues of the covariance matrix of the category 1:')\n",
    "print(np.linalg.eigvalsh(cov_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle estimate of the mean for the distribution of category 0:\n",
      "[0.97241857 1.97843254 2.96578103 4.11295877]\n",
      "mle estimate of the covariance for the distribution of category 0:\n",
      "[[ 3.13059469 -0.9204429  -0.50956809  0.03607638]\n",
      " [-0.9204429   1.31317454  0.25156013 -0.00793287]\n",
      " [-0.50956809  0.25156013  2.76000462 -0.19320975]\n",
      " [ 0.03607638 -0.00793287 -0.19320975  3.3670161 ]]\n",
      "eigenvalues of the mle of the covariance matrix for the distribution of category 0:\n",
      "[0.9276648  2.43160125 3.3599045  3.8516194 ]\n"
     ]
    }
   ],
   "source": [
    "train_data_0 = category_0.sample_data()\n",
    "pi_c_0 = 1000 / 2000\n",
    "mle_0 = MLEComputation(train_data_0, pi_c_0)\n",
    "mean_mle, cov_mle = mle_0.gaussian_mle_estimates()\n",
    "print('mle estimate of the mean for the distribution of category 0:')\n",
    "print(mean_mle)\n",
    "print('mle estimate of the covariance for the distribution of category 0:')\n",
    "print(cov_mle)\n",
    "print('eigenvalues of the mle of the covariance matrix for the distribution of category 0:')\n",
    "print(np.linalg.eigvalsh(cov_mle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle estimate of the mean for the distribution of category 1:\n",
      "[4.94527014 6.04143632 7.02131479 8.03937993]\n",
      "mle estimate of the covariance for the distribution of category 1:\n",
      "[[ 3.54744134 -1.04517846 -0.51762962  0.38783089]\n",
      " [-1.04517846  2.02292882  0.30279074 -0.35542864]\n",
      " [-0.51762962  0.30279074  3.13704031 -0.26085146]\n",
      " [ 0.38783089 -0.35542864 -0.26085146  3.56680111]]\n",
      "eigenvalues of the mle of the covariance matrix for the distribution of category 1:\n",
      "[1.48113754 2.84650426 3.27940251 4.66716727]\n"
     ]
    }
   ],
   "source": [
    "train_data_1 = category_1.sample_data()\n",
    "pi_c_1 = 1000 / 2000\n",
    "mle_1 = MLEComputation(train_data_1, pi_c_1)\n",
    "mean_mle, cov_mle = mle_1.gaussian_mle_estimates()\n",
    "print('mle estimate of the mean for the distribution of category 1:')\n",
    "print(mean_mle)\n",
    "print('mle estimate of the covariance for the distribution of category 1:')\n",
    "print(cov_mle)\n",
    "print('eigenvalues of the mle of the covariance matrix for the distribution of category 1:')\n",
    "print(np.linalg.eigvalsh(cov_mle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle of the class density measure for the class 0: 1.0842533040971799e-05\n",
      "mle of the class density measure for the class 1: 2.870042120886909e-05\n"
     ]
    }
   ],
   "source": [
    "test_input = np.array([3, 4, 5, 6])\n",
    "print('mle of the class density measure for the class 0:', mle_0.compute_class_density_measure(test_input))\n",
    "print('mle of the class density measure for the class 1:', mle_1.compute_class_density_measure(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two results, it is observed that the mle's for the probability density measures of the two classes are close to each other for the test input $\\left [3, 4, 5, 6 \\right ]$. This test input is the half-way between the means of the distributions for the two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle of the class density measure for the class 0: 0.0023380151720176056\n",
      "mle of the class density measure for the class 1: 1.9083549472588822e-10\n"
     ]
    }
   ],
   "source": [
    "mle_0 = MLEComputation(train_data_0, pi_c_0)\n",
    "mle_1 = MLEComputation(train_data_1, pi_c_1)\n",
    "test_input = np.array([1, 2, 3, 4])\n",
    "print('mle of the class density measure for the class 0:', mle_0.compute_class_density_measure(test_input))\n",
    "print('mle of the class density measure for the class 1:', mle_1.compute_class_density_measure(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test input being equal to the mean of the distribution for the class 0, the above results are in compliance with the expectation that category 0 should be chosen by the mle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle of the class density measure for the class 0: 1.1931697725230098e-12\n",
      "mle of the class density measure for the class 1: 0.0015750917957625316\n"
     ]
    }
   ],
   "source": [
    "mle_0 = MLEComputation(train_data_0, pi_c_0)\n",
    "mle_1 = MLEComputation(train_data_1, pi_c_1)\n",
    "test_input = np.array([5, 6, 7, 8])\n",
    "print('mle of the class density measure for the class 0:', mle_0.compute_class_density_measure(test_input))\n",
    "print('mle of the class density measure for the class 1:', mle_1.compute_class_density_measure(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test input being equal to the mean of the distribution for the class 1, the above results are in compliance with the expectation that category 1 should be chosen by the mle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Machine Learning A Probabilistic Perspective, Kevin P. Murphy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
